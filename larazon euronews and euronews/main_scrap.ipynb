{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"K_Ti4B9Gcba8"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import os\n","import pandas as pd\n","import warnings\n","import threading\n","import concurrent.futures\n","warnings.filterwarnings(\"ignore\")\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uqgm10KVcba-"},"outputs":[],"source":["article_counter = 1\n","counter_lock = threading.Lock()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tu4xi5WQjOy7"},"outputs":[],"source":["save_dir = r'E:\\IITGN_MTECH\\sem-3rd\\natural-lanaguage-processing\\assignment-nlp-1\\euronews_articles'\n","headers = {\n","    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pqfTWVfcba-"},"outputs":[],"source":["def save_article_to_txt(link, title, decription, author, creation_date, last_update, content, size, article_counter , save_dir):\n","    \"\"\"Save article content to a .txt file with a sequential filename.\"\"\"\n","    filename = f\"article_{article_counter}.txt\"\n","    filepath = os.path.join(save_dir, filename)\n","    print(save_dir)\n","\n","    try:\n","        # Ensure save_dir exists\n","        if not os.path.exists(save_dir):\n","            os.makedirs(save_dir)\n","\n","        # Prepare the article content for saving\n","        article_data_txt = (\n","            f\"Link: {link}\\n\"\n","            f\"Title: {title}\\n\"\n","            f\"Description: {description}\\n\"\n","            f\"Author: {author}\\n\"\n","            f\"Creation Date: {creation_date}\\n\"\n","            f\"Last Update: {last_update}\\n\"\n","            f\"Article Content: {content}\\n\"\n","            f\"Article Size: {size}B\\n\"\n","        )\n","\n","        # Write to the file\n","        with open(filepath, 'w', encoding='utf-8') as f:\n","            f.write(article_data_txt)\n","\n","        print(f\"Successfully saved article {filename}\")\n","\n","    except Exception as e:\n","        print(f\"Failed to save article {filename}: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3a8nUiucba-"},"outputs":[],"source":["def get_soup(url):\n","    \"\"\"Fetch and parse the HTML content of a URL using BeautifulSoup.\"\"\"\n","    try:\n","        # scraperapi_url = f'http://api.scraperapi.com?api_key={api_key}&url={url}'\n","        for _ in range(2):\n","              response = requests.get(url, headers=headers)\n","            #   response = requests.get(scraperapi_url, headers=headers)\n","              response.raise_for_status()  # Check for HTTP request errors\n","        if response.status_code == 200:\n","              print(\"Page retrieved successfully!\")\n","              return BeautifulSoup(response.text, 'html.parser')\n","        elif response.status_code == 429:\n","                  print(\"Rate limit exceeded. Waiting 10 seconds...\")\n","                  time.sleep(5)\n","        else:\n","              print(\"Failed to retrieve the page:\", response.status_code)\n","    except requests.exceptions.HTTPError as http_err:\n","        print(f\"Request failed: {http_err}\")\n","        return None\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UiLEcDkccba_"},"outputs":[],"source":["def extract_article_info(url ,article_soup):\n","    \"\"\"Extract required fields from the article BeautifulSoup object.\"\"\"\n","    try:\n","\n","        title_tag = article_soup.find('h1', itemprop='headline')\n","        title = title_tag.text.strip() if title_tag else \"no disponible\"\n","\n","        description_tag = article_soup.find(class_=\"ue-c-article__standfirst\")\n","        description = description_tag.text.strip() if description_tag else \"no disponible\"\n","\n","        author_tag = article_soup.find(class_=\"ue-c-article__byline-name\")\n","        author = author_tag.text.strip() if author_tag else \"no disponible\"\n","\n","        creation_date_tag = article_soup.find('meta', attrs={'property': 'article:published_time'})\n","        creation_date = creation_date_tag.get('content', 'no disponible') if creation_date_tag else \"no disponible\"\n","\n","        last_update_tag = article_soup.find('meta', attrs={'property': 'article:modified_time'})\n","        last_update = last_update_tag.get('content', 'no disponible') if last_update_tag else \"no disponible\"\n","\n","        content = \"\"\n","        content_div = article_soup.find(class_=\"NormalTextoNoticia\")\n","\n","        if content_div:\n","            paragraphs = content_div.find_all('p')\n","            content += \"\\n\".join([p.get_text(strip=True) for p in paragraphs]) if paragraphs else \"no disponible\"\n","        else:\n","            content = \"no disponible\"\n","\n","        article_size = len(content.encode('utf-8'))\n","        # print(title , content , article_size)\n","        return title, description, author, creation_date, last_update, content, article_size\n","\n","    except Exception as e:\n","        print(f\"Failed to extract information: {e}\")\n","        return None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7wMGs20gcba_"},"outputs":[],"source":["def fetch_and_process_articles(url , save_dir, article_links,start):\n","    global article_counter\n","    soup = get_soup(url)\n","    # if soup is None or is_error_page(soup):\n","    #     return\n","    try:\n","        # Safely find the container\n","        c_ = soup.find('div' , id = \"ListaUltimasNoticias\")\n","        if c_:  # Ensure the parent container is found\n","            c__ = c_.findAll('article')\n","            for content_item in c__:\n","                content_item = content_item.find(class_='flipper')\n","                content_item = content_item.find(class_='articulo-interior')\n","                content_item = content_item.find(class_ = 'articulo-titulo')\n","                article_link_soup = content_item.find('a')\n","                if article_link_soup:\n","                      article_url = article_link_soup.get('href')  # Extract the URL from 'a' tag\n","                      if article_url and article_url not in article_links:\n","                            article_links.add(article_url)\n","                            article_soup = get_soup(article_url)  # Fetch article page\n","\n","                            if article_soup:\n","                                try:\n","                                    article_info = extract_article_info(article_url ,  article_soup)\n","\n","                                    if article_info:\n","                                        title, description, author, creation_date, last_update, content, article_size = article_info\n","\n","                                        if title != \"no disponible\" and content != \"no disponible\" :\n","                                            with counter_lock:\n","                                                current_counter = article_counter\n","                                                article_counter += 1\n","                                            save_article_to_txt(article_url, title, description , author, creation_date, last_update, content, article_size, current_counter, save_dir)\n","                                            print(f\"Saved article_done: {article_url , start}\")\n","                                except Exception as e:\n","                                    print(f\"Failed to process article {article_url}: {e}\")\n","\n","    except Exception as e:\n","        print(f\"Failed to find article elements: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdwmAGstcbbA"},"outputs":[],"source":["def main():\n","    base_url = r\"https://www.europapress.es/\"\n","    start , end = 1 , 999\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n","        article_links = set()\n","        while start != end:\n","            section = f\"/p{start}/\"\n","            url = base_url + section\n","            executor.submit(fetch_and_process_articles, url, save_dir, article_links,start)\n","            start += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzdfRW5AcbbA"},"outputs":[],"source":["\n","if __name__ == \"__main__\":\n","\tmain()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":0}